{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Embedding, Linear, Dropout, MaxPool1d, Sequential, ReLU\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "\n",
    "class transformer_FFN(Module):\n",
    "    def __init__(self, emb_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.dropout = dropout\n",
    "        self.FFN = Sequential(\n",
    "                Linear(self.emb_size, self.emb_size),\n",
    "                ReLU(),\n",
    "                Dropout(self.dropout),\n",
    "                Linear(self.emb_size, self.emb_size),\n",
    "                # Dropout(self.dropout),\n",
    "            )\n",
    "    def forward(self, in_fea):\n",
    "        return self.FFN(in_fea)\n",
    "\n",
    "def ut_mask(seq_len):\n",
    "    \"\"\" Upper Triangular Mask\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(seq_len,seq_len),diagonal=1).to(dtype=torch.bool).to(device)\n",
    "\n",
    "def lt_mask(seq_len):\n",
    "    \"\"\" Upper Triangular Mask\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(seq_len,seq_len),diagonal=-1).to(dtype=torch.bool).to(device)\n",
    "\n",
    "def pos_encode(seq_len):\n",
    "    \"\"\" position Encoding\n",
    "    \"\"\"\n",
    "    return torch.arange(seq_len).unsqueeze(0).to(device)\n",
    "\n",
    "def get_clones(module, N):\n",
    "    \"\"\" Cloning nn modules\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.nn.functional as F\n",
    "from enum import IntEnum\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "from torch.nn import Module, Embedding, LSTM, Linear, Dropout, LayerNorm, TransformerEncoder, TransformerEncoderLayer, \\\n",
    "        MultiLabelMarginLoss, MultiLabelSoftMarginLoss, CrossEntropyLoss, BCELoss, MultiheadAttention\n",
    "from torch.nn.functional import one_hot, cross_entropy, multilabel_margin_loss, binary_cross_entropy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Dim(IntEnum):# 定义张量维度名称，增强代码可读性\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "class CSKT(nn.Module):\n",
    "    def __init__(self, n_question, n_pid, \n",
    "            d_model, n_blocks, dropout, d_ff=256, \n",
    "            loss1=0.5, loss2=0.5, loss3=0.5, start=50, num_layers=2, nheads=4, seq_len=512, r=1, gamma=1, \n",
    "            kq_same=1, final_fc_dim=512, final_fc_dim2=256, num_attn_heads=8, separate_qa=False, l2=1e-5, emb_type=\"qid\", emb_path=\"\", pretrain_dim=768):\n",
    "        super().__init__()\n",
    "        self.model_name = \"cskt\"\n",
    "        self.n_question = n_question\n",
    "        self.dropout = dropout\n",
    "        self.kq_same = kq_same\n",
    "        self.n_pid = n_pid\n",
    "        self.l2 = l2\n",
    "        self.model_type = self.model_name\n",
    "        self.separate_qa = separate_qa\n",
    "        self.emb_type = emb_type\n",
    "        embed_l = d_model\n",
    "        \n",
    "\n",
    "\n",
    "        self.r = r\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if self.n_pid > 0:\n",
    "            if emb_type.find(\"scalar\") != -1: \n",
    "                self.difficult_param = nn.Embedding(self.n_pid+1, 1) \n",
    "            else:\n",
    "                self.difficult_param = nn.Embedding(self.n_pid+1, embed_l)  \n",
    "            self.q_embed_diff = nn.Embedding(self.n_question+1, embed_l) \n",
    "            self.qa_embed_diff = nn.Embedding(2 * self.n_question + 1, embed_l) \n",
    "        if emb_type.startswith(\"qid\"):\n",
    "            self.q_embed = nn.Embedding(self.n_question, embed_l)\n",
    "            if self.separate_qa: \n",
    "                    self.qa_embed = nn.Embedding(2*self.n_question+1, embed_l)\n",
    "            else: \n",
    "                self.qa_embed = nn.Embedding(2, embed_l)\n",
    "        self.model = Architecture(n_question=n_question, n_blocks=n_blocks, n_heads=num_attn_heads, dropout=dropout,\n",
    "                                    d_model=d_model, d_feature=d_model / num_attn_heads, d_ff=d_ff,  kq_same=self.kq_same, model_type=self.model_type, seq_len=seq_len, \n",
    "                                    r = r, gamma=gamma)\n",
    "    \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(d_model + embed_l,\n",
    "                      final_fc_dim), nn.ReLU(), nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim, final_fc_dim2), nn.ReLU(\n",
    "            ), nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim2, 1)\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for p in self.parameters():\n",
    "            if p.size(0) == self.n_pid+1 and self.n_pid > 0:\n",
    "                torch.nn.init.constant_(p, 0.)\n",
    "\n",
    "    def base_emb(self, q_data, target):\n",
    "        q_embed_data = self.q_embed(q_data)  # BS, seqlen,  d_model# c_ct\n",
    "        if self.separate_qa:\n",
    "            qa_data = q_data + self.n_question * target\n",
    "            qa_embed_data = self.qa_embed(qa_data)\n",
    "        else:\n",
    "            # BS, seqlen, d_model # c_ct+ g_rt =e_(ct,rt)\n",
    "            qa_embed_data = self.qa_embed(target)+q_embed_data\n",
    "        return q_embed_data, qa_embed_data\n",
    "\n",
    "    def get_attn_pad_mask(self, sm):\n",
    "        batch_size, l = sm.size()\n",
    "        pad_attn_mask = sm.data.eq(0).unsqueeze(1)\n",
    "        pad_attn_mask = pad_attn_mask.expand(batch_size, l, l)\n",
    "        return pad_attn_mask.repeat(self.nhead, 1, 1)\n",
    "    def forward(self, dcur, qtest=False, train=False):\n",
    "        q, c, r = dcur[\"qseqs\"].long(), dcur[\"cseqs\"].long(), dcur[\"rseqs\"].long()\n",
    "        qshft, cshft, rshft = dcur[\"shft_qseqs\"].long(), dcur[\"shft_cseqs\"].long(), dcur[\"shft_rseqs\"].long()\n",
    "        pid_data = torch.cat((q[:,0:1], qshft), dim=1).to(device)\n",
    "        q_data = torch.cat((c[:,0:1], cshft), dim=1).to(device)\n",
    "        target = torch.cat((r[:,0:1], rshft), dim=1).to(device)\n",
    "\n",
    "        emb_type = self.emb_type\n",
    "\n",
    "        if emb_type.startswith(\"qid\"):\n",
    "            q_embed_data, qa_embed_data = self.base_emb(q_data, target)\n",
    "        if self.n_pid > 0 and emb_type.find(\"norasch\") == -1: # have problem id\n",
    "        \n",
    "            if emb_type.find(\"aktrasch\") == -1:\n",
    "                q_embed_diff_data = self.q_embed_diff(q_data)  # \n",
    "                pid_embed_data = self.difficult_param(pid_data)  # \n",
    "                q_embed_data = q_embed_data + pid_embed_data * \\\n",
    "                    q_embed_diff_data \n",
    "\n",
    "            else:\n",
    "                q_embed_diff_data = self.q_embed_diff(q_data)  # \n",
    "                pid_embed_data = self.difficult_param(pid_data)  # \n",
    "                q_embed_data = q_embed_data + pid_embed_data * \\\n",
    "                    q_embed_diff_data \n",
    "\n",
    "                qa_embed_diff_data = self.qa_embed_diff(\n",
    "                    target)  # \n",
    "                qa_embed_data = qa_embed_data + pid_embed_data * \\\n",
    "                        (qa_embed_diff_data+q_embed_diff_data)  \n",
    "        y2, y3 = 0, 0\n",
    "        if emb_type in [\"qid\", \"qidaktrasch\", \"qid_scalar\", \"qid_norasch\"]:\n",
    "            d_output = self.model(q_embed_data, qa_embed_data)\n",
    "\n",
    "            concat_q = torch.cat([d_output, q_embed_data], dim=-1)\n",
    "            output = self.out(concat_q).squeeze(-1)\n",
    "            m = nn.Sigmoid()\n",
    "            preds = m(output)\n",
    "\n",
    "        if train:\n",
    "            return preds, y2, y3\n",
    "        else:\n",
    "            if qtest:\n",
    "                return preds, concat_q\n",
    "            else:\n",
    "                return preds\n",
    "\n",
    "class Architecture(nn.Module):\n",
    "    def __init__(self, n_question,  n_blocks, d_model, d_feature,\n",
    "                 d_ff, n_heads, dropout, kq_same, model_type, seq_len, r, gamma):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            n_block : number of stacked blocks in the attention\n",
    "            d_model : dimension of attention input/output\n",
    "            d_feature : dimension of input in each of the multi-head attention part.\n",
    "            n_head : number of heads. n_heads*d_feature = d_model\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type in {'cskt'}:\n",
    "            self.blocks_2 = nn.ModuleList([\n",
    "                TransformerLayer(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                                 d_ff=d_ff, dropout=dropout, n_heads=n_heads, kq_same=kq_same, seq_len = seq_len, r=r, gamma=gamma)\n",
    "                for _ in range(n_blocks)\n",
    "            ])\n",
    "\n",
    "    def forward(self, q_embed_data, qa_embed_data):\n",
    "        seqlen, batch_size = q_embed_data.size(1), q_embed_data.size(0)\n",
    "\n",
    "\n",
    "        qa_pos_embed = qa_embed_data\n",
    "        q_pos_embed = q_embed_data\n",
    "\n",
    "        y = qa_pos_embed\n",
    "        seqlen, batch_size = y.size(1), y.size(0)\n",
    "        x = q_pos_embed\n",
    "\n",
    "        \n",
    "        for block in self.blocks_2:\n",
    "            x = block(mask=0, query=x, key=x, values=y, apply_pos=True) #\n",
    "            \n",
    "        return x\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_feature,\n",
    "                 d_ff, n_heads, dropout,  kq_same, seq_len, r, gamma):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            This is a Basic Block of Transformer paper. It containts one Multi-head attention object. Followed by layer norm and postion wise feedforward net and dropout layer.\n",
    "        \"\"\"\n",
    "        kq_same = kq_same == 1\n",
    "\n",
    "        self.masked_attn_head = MultiHeadAttention(\n",
    "            d_model, d_feature, n_heads, dropout, kq_same=kq_same, seq_len=seq_len, r=r, gamma=gamma)\n",
    "\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, mask, query, key, values, apply_pos=True):\n",
    "        seqlen, batch_size = query.size(1), query.size(0)\n",
    "        nopeek_mask = np.triu(\n",
    "            np.ones((1, 1, seqlen, seqlen)), k=mask).astype('uint8')\n",
    "        src_mask = (torch.from_numpy(nopeek_mask) == 0).to(device)\n",
    "        if mask == 0: \n",
    "            query2 = self.masked_attn_head(\n",
    "                query, key, values, mask=src_mask, zero_pad=True) \n",
    "        else:\n",
    "            query2 = self.masked_attn_head(\n",
    "                query, key, values, mask=src_mask, zero_pad=False)\n",
    "\n",
    "        query = query + self.dropout1((query2)) #\n",
    "        query = self.layer_norm1(query) \n",
    "        if apply_pos:\n",
    "            query2 = self.linear2(self.dropout( \n",
    "                self.activation(self.linear1(query))))\n",
    "            query = query + self.dropout2((query2)) # \n",
    "            query = self.layer_norm2(query) \n",
    "        return query\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, max_len, num_heads): \n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.rel_pos_bias = nn.Parameter(torch.zeros(num_heads, 2 * max_len - 1))\n",
    "        nn.init.normal_(self.rel_pos_bias, std=0.02) \n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        device = self.rel_pos_bias.device\n",
    "        range_vec = torch.arange(seq_len, device=device) \n",
    "        distance_mat = range_vec[None, :] - range_vec[:, None] \n",
    "        distance_mat_clipped = distance_mat + self.max_len - 1 \n",
    "        assert distance_mat_clipped.min() >= 0 and distance_mat_clipped.max() < 2 * self.max_len - 1\n",
    "\n",
    "        values = self.rel_pos_bias[:, distance_mat_clipped] \n",
    "        return values  \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout, kq_same, r, gamma, seq_len=512,bias=True):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        It has projection layer for getting keys, queries and values. Followed by attention and a connected layer.\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_feature\n",
    "        self.h = n_heads\n",
    "        self.kq_same = kq_same\n",
    "\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        if kq_same is False:\n",
    "            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.num_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_bias = bias\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.seq_len = seq_len\n",
    "        self.r = r\n",
    "        self.gamma = gamma\n",
    "        self.rel_pos_bias_module = RelativePositionBias(self.seq_len, self.num_heads)\n",
    "        self.kernel_bias = ParallelKerpleLog(self.h)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_uniform_(self.k_linear.weight)\n",
    "        xavier_uniform_(self.v_linear.weight)\n",
    "        if self.kq_same is False:\n",
    "            xavier_uniform_(self.q_linear.weight)\n",
    "\n",
    "        if self.proj_bias:\n",
    "            constant_(self.k_linear.bias, 0.)\n",
    "            constant_(self.v_linear.bias, 0.)\n",
    "            if self.kq_same is False:\n",
    "                constant_(self.q_linear.bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def forward(self, q, k, v, mask, zero_pad):\n",
    "\n",
    "        bs = q.size(0)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        if self.kq_same is False:\n",
    "            q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        else:\n",
    "            q = self.k_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        rel_pos_bias_module = RelativePositionBias(self.seq_len, self.num_heads)\n",
    "        rel_pos_emb_tensor = rel_pos_bias_module(self.seq_len) \n",
    "        scores = attention(q, k, v, self.d_k,\n",
    "                   mask, self.dropout, zero_pad, self.r, self.gamma, self.kernel_bias, self.rel_pos_bias_module, alpha=0.5)\n",
    "    \n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out_proj(concat)\n",
    "\n",
    "        return output\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def disentangled_attention_scores(q, k, rel_pos_bias_module, mask=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))  \n",
    "    seq_len = q.size(-2)\n",
    "    rel_pos_emb = rel_pos_bias_module(seq_len)  # (num_heads, seq_len, seq_len)\n",
    "\n",
    "    assert rel_pos_emb.size(0) == scores.size(1), f\"rel_pos_emb heads {rel_pos_emb.size(0)} != scores heads {scores.size(1)}\"\n",
    "    scores = scores + rel_pos_emb.unsqueeze(0)  # (1, head, seq_len, seq_len)\n",
    "    scores = scores / (q.size(-1) ** 0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "        # 确保mask形状匹配 (batch, 1, seq_len, seq_len)\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 防止全部-∞导致softmax输出nan，先确保mask不全是0\n",
    "        mask_sum = mask.sum(dim=-1, keepdim=True)\n",
    "        mask = torch.where(mask_sum == 0, torch.ones_like(mask), mask)\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    return scores\n",
    "\n",
    "def retnet_retention(q, k, v, mask, decay=0.9):\n",
    "    batch, head, seq_len, d_k = q.size()\n",
    "    output = torch.zeros_like(v) # output：最终输出序列\n",
    "    y = torch.zeros(batch, head, d_k).to(q.device) # y：用来保存前面累积的信息（初始化为0）\n",
    "\n",
    "    # 保证 decay 在 [0, 1] 范围内\n",
    "    decay = max(min(decay, 1.0), 0.0)\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        x_t = v[:, :, t, :]\n",
    "        y = decay * y + x_t\n",
    "        output[:, :, t, :] = y\n",
    "\n",
    "    # 如果mask非空，置0无效位置\n",
    "    if mask is not None:\n",
    "        # mask: (batch, 1, seq_len, seq_len)，对t对应维度置0\n",
    "        mask_t = mask[:, :, t, :].squeeze(1) if mask.dim() == 4 else None\n",
    "        if mask_t is not None:\n",
    "            output = output * mask_t.unsqueeze(-1).float()\n",
    "\n",
    "    return output\n",
    "\n",
    "def disentangled_retnet_attention(q, k, v, mask, dropout, rel_pos_bias_module, alpha=0.5):\n",
    "    \"\"\"结合传统注意力和 RetNet 机制\"\"\"\n",
    "    scores = disentangled_attention_scores(q, k, rel_pos_bias_module, mask)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    attn = dropout(attn)\n",
    "    attn_output = torch.matmul(attn, v)\n",
    "\n",
    "    ret_output = retnet_retention(q, k, v, mask)\n",
    "\n",
    "    output = attn_output + alpha * ret_output\n",
    "    return output\n",
    "\n",
    "\n",
    "def attention(q, k, v, d_k, mask, dropout, zero_pad, r, gamma, kernel_bias, rel_pos_bias_module, alpha=0.5):\n",
    "    output = disentangled_retnet_attention(q, k, v, mask, dropout, rel_pos_bias_module, alpha)\n",
    "\n",
    "    if zero_pad:\n",
    "        bs, head, seqlen = output.size(0), output.size(1), output.size(2)\n",
    "        pad_zero = torch.zeros(bs, head, 1, output.size(-1)).to(q.device)\n",
    "        output = torch.cat([pad_zero, output[:, :, 1:, :]], dim=2)\n",
    "\n",
    "    return output\n",
    "\n",
    "# def attention(q, k, v, d_k, mask, dropout, zero_pad, r, gamma, kernel_bias, rel_pos_bias_module, alpha=0.5):\n",
    "#     \"\"\"只有retnet\"\"\"\n",
    "#     return retnet_retention(q, k, v, mask)\n",
    "\n",
    "# def attention(q, k, v, d_k, mask, dropout, zero_pad, r, gamma, kernel_bias, rel_pos_bias_module, alpha=0.5):\n",
    "#     \"\"\"disentangled_attention_only+相对位置偏置\"\"\"\n",
    "#     scores = disentangled_attention_scores(q, k, rel_pos_bias_module, mask)\n",
    "#     attn = F.softmax(scores, dim=-1)\n",
    "#     attn = dropout(attn)\n",
    "#     return torch.matmul(attn, v)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7460856,
     "sourceId": 11872031,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
